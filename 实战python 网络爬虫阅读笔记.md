# 实战python 网络爬虫

[TOC]



## 阅读笔记

书籍阅读笔记

《实战python 网络爬虫》，这本书很好，因为介绍了python爬虫所需要的知识框架，对于涉及到的方方面面的信息都进行了介绍，而且介绍的内容深度是刚刚合适的点到为止，易于理解。

**本书结构**
本书共分28章，各章内容概述如下：
第1章介绍什么是网络爬虫、爬虫的类型和原理、爬虫搜索策略和爬虫的合法性及开发流程。
第2章讲解爬虫开发的基础知识，包括HTTP协议、请求头和Cookies的作用、HTML的布局结构、JavaScript的介绍、JSON的数据格式和Ajax的原理。
第3章介绍使用Chrome开发工具分析爬取网站，重点介绍开发工具的Elements和Network标签的功能和使用方式，并通过开发工具分析QQ网站。
第4章主要介绍Fiddler 抓包工具的原理和安装配置，Fiddler用户界面的各个功能及使用方法。
第5章讲述了Urllib在Python2和Python3的变化及使用，包括发送请求、使用代理IP、Cookies的读写、HTTP证书验收和数据处理。
第6章~第8章介绍Python第三方库Requests、Requests-Cache爬虫缓存和Requests-HTML，包括发送请求、使用代理IP、Cookies的读写、HTTP证书验收和文件下载与上传、复杂的请求方式、缓存的存储机制、数据清洗以及Ajax动态数据爬取等内容。
第9章介绍网页操控和数据爬取，重点讲解Selenium的安装与使用，并通过实战项目“百度自动答题”，讲解了Selenium的使用。
第10章介绍手机App数据爬取，包括Appium的原理与开发环境搭建、连接Android系统，并通过实战项目“淘宝商品采集”，介绍了App数据的爬取技巧。
第11章介绍Splash、Mitmproxy与Aiohttp的安装和使用，包括Splash动态数据抓取、Mitmproxy抓包和Aiohttp高并发抓取。
第12章介绍验证码的种类和识别方法，包括OCR的安装和使用、验证码图片处理和使用第三方平台识别验证码。第13章讲述数据清洗的三种方法，包括字符串操作（截取、查找、分割和替换）、正则表达式的使用和第三方库BeautifulSoup的安装以及使用。
第14章讲述如何将数据存储到文件，包括CSV、Excel和Word文件的读取和写入方法。第15章介绍ORM框架SQLAlchemy的安装及使用，实现关系型数据库持久化存储数据。
第16章讲述非关系型数据库MongoDB的操作，包括MongoDB的安装、原理和Python实现MongoDB的读写。
第17章至第21章介绍了5个实战项目，分别是：爬取51Job招聘信息、分布式爬虫一-QQ音乐、12306抢票爬虫、微博爬取和微博爬虫软件的开发。
第22章至第25章介绍了Scrapy爬虫框架，包括Scrapy的运行机制、项目创建、各个组件的编写（Setting、Items、Item Pipelines和Spider）和文件下载及Scrapy中间件，并通过实战项目
“Scrapy+Selenium爬取豆瓣电影评论”、“Scrapy+Splash爬取B站动漫信息”和“Scrapy+Redis分布式爬取猫眼排行榜”、“爬取链家楼盘信息”和“QQ音乐全站爬取”，深入讲解了Scrapy的应用和分布式爬虫的编写技巧。
第26章介绍爬虫的上线部署，包括非框架式爬虫和框架式爬虫的部署技巧。
第27章介绍常见的反爬虫技术，并给出了可行的反爬虫解决方案。
第28章介绍爬虫框架的编写，学习如何自己动手编写一款爬虫框架，以满足特定业务场景的需求。

## 爬虫定义

网络爬虫是一种按照一定的规则自动地抓取网络信息的程序或者脚本。简单来说，网络爬虫就是根据一定的算法实现编程开发，主要通过URL实现数据的抓取和发掘。

网络爬虫根据系统结构和开发技术大致可以分为4种类型：通用网络爬虫、聚焦网络爬虫、增量式网络爬虫和深层网络爬虫。网络爬虫的类型理论上分为4类，但实际上主要是两大类：通用爬虫和聚焦爬虫。通用爬虫主要有Google、百度、必应等搜索引擎，主要以核心算法为主导，学习成本相对较高。聚焦爬虫就是定向爬取数据，是有目的性的爬虫，学习成本相对较低。

## 爬虫的合法性

网络爬虫在大多数情况下都不会违法，在生活中几乎都有爬虫应用，比如在百度中搜索的内容几乎都是通过爬虫采集下来的，因此网络爬虫作为一门技术，技术本身是不违法的，且在大多数情况下可以放心使用爬虫技术。当然也有特殊情况，正如水果刀本身在法律上并不被禁止使用，但是用来伤害他人，这就触犯了法律规则。一般情况下，爬虫所带来的违法风险主要体现在以下几个方面：
（1）利用爬虫技术与黑客技术结合，攻击网站后台，从而窃取后台数据。因为爬虫是爬取网站上的网页信息，这些信息能给用户浏览，也就是说这些信息允许我们使用和爬取。但网站的后台数据是不被公开的数据，这些数据涉及了用户的隐私和财产安全，如果通过爬虫技术与黑客技术窃取后台数据，这就明显触发法律的底线。
（2）利用爬虫恶意攻击网站，造成网站系统的瘫痪。爬虫是通过程序去访问并操控网站，因此访问速度非常快，再加上程序的高并发处理，可以在短时间内模拟成千上万的用户在访问网站。当网站的访问量过高，就会加重网站的负载，从而造成系统的瘫痪，如果长期这样恶意攻击网站系统，也很可能违反相关的法律条例。
综上所述，爬虫技术本身是无罪的，问题往往出在人的无限欲望上。因此爬虫开发者和企业经营者的道德良知才是避免触碰法律底线的根本所在。

## 爬虫的开发流程

一般情况下，爬虫的开发流程如下：
（1）需求说明。任何程序开发都离不开需求说明，爬虫开发也是如此。需求说明包含功能说明、功能的业务逻辑等详细说明。爬虫的需求说明要明确告知开发人员需要爬取哪些数据、数据的存储方式以及爬虫的爬取效率。
（2）爬虫开发计划。根据爬虫的需求说明制定相关的开发计划，比如选择爬虫的开发工具、功能模块化设计、设计爬虫运行模式等一系列开发明细。
（3）爬虫的功能开发。根据开发计划编写相应的功能代码。以功能模块化设计为依据，每个功能模块以函数或类的形式表示，再将各个模块进行组合，从而实现整个爬虫功能的开发。
（4）爬虫的部署与交付。程序开发完成后（包含测试通过）就可以进行部署上线或交付客户。部署和交付的方式有多种，比如打包exe程序、GUI界面（爬虫软件）或定时执行等。

## 爬虫相关的Web前端开发技术

前端开发技术是爬虫开发人员必备技能之一，也是编写爬虫程序的基础。前端技术的主要作用是分析各类网站的设计架构，以便有针对性地编写爬虫脚本。从整个爬虫开发周期来看，分析网站架构是最为耗时的一环，也是爬虫开发的核心之一，可以说，爬虫的开发都是基于网站的分析为前提。

关于前端开发技术，读者应重点掌握以下内容。
* HTTP与HTTPS：互联网上应用最为广泛的一种网络协议。目前所有网站开发都基于该协议，也是网站的实现原理。
* 请求头：基于HTTP与HTTPS协议实现，其作用是在通信之间实现信息传递。熟知各种请求类型，对爬虫中编写请求头有指导性作用。
* Cookies：存储在用户主机浏览器中的文本文件，主要让服务器识别各个用户身份信息。
* HTML：服务器返回的网页内容，一般由服务器后台生成。网站大部分数据来源于此，熟悉HTML布局和各个标签的作用，有利于数据抓取和清洗。
* JavaScript：主要实现网页的动态功能及用户交互。要懂得分析JavaScript代码，尤其是数据加密处理。
* JSON：表示一个JavaScript对象的信息，本质是一个特殊的字符串。·Ajax：主要是前端数据加载和渲染技术，其响应内容大部分以JSON格式为主。

## Chrome 分析网站

Chrome开发者工具的主要作用是进行Web开发调试，对于爬虫开发人员来说，应该熟练掌握Elements、Console和Network。其中Network是核心部分，百分之九十的网站分析都在Network上完成，读者对Network上的各个功能和作用要理解掌握，并懂得如何使用Chrome分析网站的请求信息。
一般分析网站最主要的是找到数据的来源，确定数据来源就能确定数据生成的具体方法。总结归纳分析网站的步骤如下：
（1）找出数据来源，大部分数据来源于Doc、XHR和JS标签。
（2）找到数据所在的请求，分析其请求链接、请求方式和请求参数。
（3）查找并确定请求参数来源。有时候某些请求参数是通过另外的请求生成的，比如请求A的参数id是通过请求B所生成的，那么要获取请求A的数据，就要先获取请求B的数据作为A的请求参数。

## Fiddler抓包

Fiddler是一款非常流行并且实用的HTTP抓包工具，它的原理是在电脑中开启一个HTTP代理服务器，然后转发所有的HTTP请求和响应。因此，比一般的浏览器自带的抓包工具（开发者工具）要好用得多。不仅如此，Fiddler还可以支持请求重放等一些高级功能，也可以支持对手机应用进行HTTP抓包。

Fiddler提供了Windows环境下的.exe安装包，使其安装极其简单方便。安装完成后，需配置HTTPS抓取功能和手机抓包功能，完成配置便可对HTTPS网站和手机进行抓包。
除了功能强大之外，在使用上也较为简单，使用者只要打开Fiddler，然后在浏览器（手机）中进行操作，Fiddler就会自动抓取请求信息。就Fiddler本身的功能而言，使用者只需熟知每个功能按钮的作用便知道如何使用。
对于爬虫开发人员来说，需要掌握Web Session列表和View常用选项视图的基本功能，能够分析并得知每个请求的类型、状态码、请求方式、请求头、请求链接、请求参数以及响应内容等基本信息。

## 爬虫库Urllib

在Python3中，Urllib是一个收集几个模块来使用URL的软件包，大致具备以下功能。

其常用的语法有以下几种。

* urllib.request.urlopen:urllib最基本的使用功能，用于访问URL（请求链接）的唯一方法。
* urllib.request.Request：声明request对象，该对象可自定义请求头（header）、请求方式等信息。
* urllib.request.ProxyHandler：动态设置代理IP池，可加载请求对象。
* urllib.request.HTTPCookieProcessor：设置Cookies对象，可加载请求对象。
* urllib.request.build openerO：创建请求对象，用于代理IP和Cookies对象加载。
* urllib.parse.urlencode(data).encode('utf-8')：请求数据格式转换。
* urllib.parse.quote（url）：URL编码处理，主要对URL上的中文等特殊符号编码处理。
* urllib.parse.unquote（url）：URL解码处理，将URL上的特殊符号还原。除了Urllib之外，一些特殊请求需要结合其他模块配合使用，如Cookies读写由HTTP模块完成，关闭证书验证需要SSL模块设置，等等。

## 爬虫库Requests

Requests是Python的一个很实用的HTTP客户端库，完全满足如今网络爬虫的需求。与Urllib对比，Requests不仅具备Urlib的全部功能；在开发使用上，语法简单易懂，完全符合Python优雅、简洁的特性；在兼容性上，完全兼容Python2和Python3，具有较强的适用性。

读者要掌握Requests实现GET和POST请求时分别使用了不同的方法，如下代码所示：

```
import requests 
url='https://baidu.com/'
#GET请求
r=requests.get(url，headers=headers，proxies=proxies，verify=False，cookies=cookies)
#POST请求
r=requests.post(url，data=data，files=files，headers=headers，proxies=proxies，verify=False，cookies=cookies)
```

Requests的GET和POST将请求中所需要使用的功能都以参数的形式直接作用到请求中。一个发送请求的语句就已包含了请求头、代理IP、Cookies、证书验证、文件上传等功能。
另外，Requests还提供了r.status code、r.raw、r.content、r.text、r.headers、r.json()、r.raise for status()、r.url、r.cookies、r.encoding 10种方法获取响应内容。

## Requests-Cache 爬虫缓存

Requests-Cache是Requests模块的一个扩展功能，它是根据Requests的发送请求来生成相应的缓存数据。当Requests重复向同一个URL发送请求的时候，Requests-Cache会判断当前请求是否已产生缓存，若已有缓存，则从缓存里读取数据作为响应内容；若没有缓存，则向网站服务器发送请求，并将得到的响应内容写入相应的数据库里。

Requests-Cache的作用非常重要，它可以减少网络资源重复请求的次数，不仅减轻了本地的网络负载，而且还减少了爬虫对网站服务器的请求次数，这也是解决反爬虫机制的一个重要手段。安装Requests-Cache可以通过pip指令完成。

整个缓存机制由install cache()方法实现，该方法的参数说明如下。

* cache name：默认值为cache，这是对缓存的存储文件进行命名。
* backend：设置缓存的存储机制，默认值为None，即默认sqlite数据库存储。
* expire after：设置缓存的有效时间，默认值None，即为永久有效。
* allowable codes：设置HTTP的状态码，默认值为200。
* allowablemethods：设置请求方式，默认值是只允许GET请求才能生成缓存。
* session factory：设置缓存的执行对象，由CachedSession类实现，该类是由Requests-Cache定义。
* backend options：设置存储配置，若缓存的存储选择sqlite、redis或mongoDB数据库，则该参数是设置数据库的连接方式。
* Requests-Cache 支持4种不同的存储机制：memory、sqlite、redis和mongoDB，4种存储机制说明如下。
* memory：每次程序运行都会将缓存以字典的形式保存在内存中，程序运行完毕，缓存也随之销毁。
* sqlite：将缓存存储在sqlite数据库，这是Requests-Cache默认的存储机制。
* redis：将缓存存储在redis数据库，通过redis模块实现数据库的读写。
* mongoDB：将缓存存储在mongoDB数据库，通过pymongo模块实现数据库的读写。

## 爬虫库Requests-HTML

* Requests-HTML是在Requests的基础上进一步封装，这两个爬虫库都是由同一个开发者开发的。Requests-HTML除了包含Requests的所有功能之外，还新增了数据提取和Ajax数据动态渲染。

* Requests-HTML只能使用Requests的Session模式，该模式是将请求会话实现持久化，可使这个请求保持连接状态。Session模式好比我们打电话，只要双方没有挂断电话，就会一直保持一种会话（连接）状态。

* 数据提取是由lxml和PyQuery模块实现，这两个模块分别支持XPath Selectors和CSS Selectors定位，通过XPath或CSS定位，可以精准地提取网页里的数据。

* Ajax数据动态渲染是将网页的动态数据加载到网页上再抓取，它是由Requests-HTML的render()方法实现，通过调用Chromium浏览器来加载Ajax功能，从而实现网页信息加载。

## 网页操控与数据爬取

Selenium是一个用于网站应用程序自动化的工具，它可以直接运行在浏览器中，就像真正的用户在操作一样。它支持的浏览器包括E、Mozila Firefox、Safari、Google Chrome和Opera等，同时支持多种编程语言，如.Net、Java、Python和Ruby等。
搭建Selenium 开发环境需要安装 Selenium库并且配置Google Chrome的WebDriver。安装Selenium库可以使用pip指令完成；配置Google Chrome的WebDriver首先通过浏览器版本确认WebDriver的版本，然后下载相应的WebDriver并存放在Python的安装目录。
Selenium定位网页元素主要通过元素的属性值或者元素在HTL里的路径位置，定位方式有以下8种：

```python
#通过属性id和name来实现定位
find element by id（）
find element by name（）
#通过HTML标签类型和属性c1ass实现定位find element by class name（）
find element by tag name（）
#通过标签值实现定位，partial link用于模糊匹配。
find element by link text（）
find element by partial link text（）
#元素的路径定位选择器
find element by xpath（）
find element by css selector（）
```

Selenium可以模拟任何操作，比如单击、右击、拖拉、滚动、复制粘贴或者文本输入等。操作方式分为三大类：常规操作、鼠标事件操作和键盘事件操作。
Selenium还有一些常用功能，如设置浏览器的参数、浏览器多窗口切换、设置等待时间、文件的上存与下载、Cookies处理以及frame框架操作。

实例：百度知道自动答题

## 手机APP数据抓取

Appium是一个开源、跨平台的测试框架，可以用来测试原生及混合的移动端应用，支持iOS、Android及FirefoxOS平台，但利用Appium的定位功能也可实现数据爬取。

## Splash、Mitmproxy与Aiohttp

（1）Splash是具有JavaScript渲染功能并带有HTTPAPI的轻量级浏览器，同时还对接了Python的网络引擎框架Twisted和QT库。简单来说，Splash是一个带有API接口的轻量级浏览器，通过使用它提供的API接口可以简单实现Ajax动态数据的抓取。
Splash最大的作用是可以执行JavaScript代码，将Ajax动态数据直接加载到网页上，无需开发者花费时间和精力分析Ajax请求，从而实现相关数据的抓取。
Python可以使用Splash提供的API接口，从而实现Python与Splash之间的交互。Splash提供多种API接口实现不同的功能，本书只列出了一些较为常用的API接口及使用方法。

（2）Mitmproxy是一个支持HTTP和HTTPS的抓包程序，实现的功能与Fiddler抓包工具相同，只不过它是以控制台的形式操作。Mitmproxy还有两个关联组件：Mitmweb与Mitmdump，前者是一个基于Web的Mitmproxy接口，它可以清楚地观察Mitmproxy捕获的请求与响应；后者是Mitmproxy的命令行接口，它可以帮助我们对接Python脚本，使用Python实现监听后的处理。
Mitmproxy还具备以下功能：

* 拦截HTTP和HTTPS请求和响应，并允许动态修改请求和响应。
* 保存完整的HTTP会话，以便重播和分析。·可重播HTTP会话的客户端（即手机端）。
* 记录所有HTTP响应。
* 反向代理模式，用于将流量转发到指定的服务器。·MacoS和Linux系统具有透明代理模式。
* 使用Python对HTTP流量进行脚本操作。
* 用于拦截的SSL/TLS证书是即时生成。

总的来说，Mitmproxy的功能是十分强大，而在爬虫开发中，它可以帮助我们对接Python脚本，就这一功能已经能帮助我们解决爬虫中常见的问题，比如爬取手机App应用的图片、视频、文字内容等。Appium侧重于手机的自动化操控，对App的图片、视频、文字内容爬取相对困难，而Mitmproxy则补充了Appium的缺点，可以说Appium+Mitmproxy是手机App爬虫开发的利器。

（3）Aiohttp是Python的一个第三方网络编程模块，它可以开发服务端和客户端，服务端也就说我们常说的网站服务器；客户端是访问网站的API接口，常用于接口测试，也可用于开发网络爬虫。Aiohtp是基于Asyncio实现的HTTP框架，Asyncio是从Python3.4开始引入的标准库，它是因为协程的概念而生，这是Python官网推荐高并发的模块之一。
Aiohttp的使用方法与Requests模块具有相似之处，而且Aiohttp具有异步并发的特性，可以轻松实现高并发的爬虫开发。

## 验证码识别

### 1. 验证码识别方法

在开发爬虫时，经常会遇到验证码识别，在网站中加入验证码的目的是加强用户安全性和提高反爬虫机制，有效防止对某一个特定注册用户用特定程序暴力破解的方式不断地进行登录尝试。

现在大多数网站还使用字符验证码，主要用于用户登录。有些网站数据需要用户登录才有访问权限，爬取这样的数据时，首先要完成用户登录，获取访问权限才能继续下一步的数据爬取。
对于用户登录设置验证码识别的网站有三种解决方案：
（1）人工识别验证码。将验证码图片下载到本地，然后靠使用者自行识别并将识别内容输入，程序获取输入内容，完成用户登录。其特点是开发简单，适合初学者，但过分依赖人为控制，难以实现批量爬取。
（2）通过Python调用OCR引擎识别验证码。这是最理想的解决方案，但正常情况下，OCR准确率较低，需要机器学习不断提高OCR准确率，开发成本相对较高。
（3）调用API使用第三方平台识别验证码。开发成本较低，有完善的API接口，直接调用即可，识别准确率高，但每次识别需收取小额费用。
上述方案是目前解决验证码最有效的手段，本章主要介绍如何使用OCR技术和第三方平台识别验证码。

### 2.OCR 

OCR是指使用电子设备（例如扫描仪或数码相机）检查纸上打印的字符，通过检测暗、亮的模式确定其形状，然后用字符识别方法将形状翻译成计算机文字的过程；即针对印刷体字符，采用光学的方式将纸质文档中的文字转换成为黑白点阵的图像文件，并通过识别软件将图像中的文字转换成文本格式，供文字处理软件进一步编辑加工的技术。
Python中支持的ORC模块有pytesser3和pyocr，其原理主要是通过模块功能调用OCR引擎识别图片，OCR引擎再将识别的结果返回到程序中。

### 3.验证码识别平台

验证码识别平台主要有以下两种：

（1）打码平台：主要由在线人员识别验证码。开发者只需调用平台API接口，一般在10秒内返回结果。识别错误或者无法识别不收费。
（2）AI开发者平台：主要由人工智能系统识别，准确率取决于系统的智能程度。调用API接口使用，每天有免费使用次数，也可付费使用，目前主流平台有百度AI和腾讯AI。

## 数据清洗

数据清洗是爬虫开发重要的一环，主要衔接了数据爬取和数据入库。

常用的数据清洗方法有：字符串操作、正则表达式和第三方模块（库）。
常用数据清洗的字符串操作有截取、替换、查找和分割。·截取：字符串[开始位置：结束位置：间隔位置]。

* 替换：字符串.replace被替换内容”，替换后内容）。
* 查找：字符串.find要查找的内容’[，开始位置，结束位置]）。
* 分割：字符串.split（分割符1，分割次数）。
### 正则表达式包含正则语法和正则处理函数。
* 正则语法：也称元字符，这类符号代表正则规则，通常表示一些不寻常的匹配操作，或者通过重复、修改匹配意义来影响正则模式的其他部分。
* 正则处理函数：Python的正则模块是re，该模块含有多种正则处理函数，功能函数包括：
        （1）re.match（patterm，string，flags=0）
        （2）re.search（pattern，string，flags=0）
        （3）re.findall（pattern，string，flags=0）
        （4）re.sub（pattern，repl，string，count=0，flags=0）
        （5）re.split（pattern，string[，maxsplit]）
        （6）re.subn（pattern，repl，string[，count]）
        （7）re.escape（string）
        （8）re.finditer（pattern，string[，flags]）
### BeautifulSoup

是一个可以从HTL或X1ML文件中提取数据的Python库。常用的数据清洗函数如下。

* 查找全部标签：soup.a，返回第一个标签a。
* 获取定位元素（标签）的值：soup.a.getText()，获取第一个标签a的值。
* 获取标签属性：soup.a['href']，获取整个HTML第一个标签a的href属性值。
* 精准查找：find_all()和find()。
    * 属性定位：soup.find_all('a', id = "try3")。
    * 多属性定位：soup.find_all('a', class_ = "efg"，id = "try3"）。
* 正则表达式模糊匹配：soup.findall（a', href == re.compile("aaa")。
* CSS选择器。
    * 通过id查找：soup.select(“#try3")。
    * 通过class查找：soup.select(".efg")。
    * 通过属性查找：soup.select('a[class="efg"]')。

## 文档数据存储

写入和读取CSV、Excel和Word中的数据是编写爬虫程序的重要内容，存入CSV、Excel和Word中的数据一般具体动态变化性质，有一定的时效性，适用于股市行情、商品信息、新闻报道和排行榜信息等。

本章主要讲解了CSV、Excel和Word中数据的写入和读取方法，要点如下：
### 1.CSV写入数据的整体思路：
（1）open函数打开CSV文件，模式为w（一般设置newline="），生成file对象。
（2）CSV模块的writer()方法加载对象file。
（3）使用writerow(), writerows() 写入一行（多行）数据。

### 2.CSV读取数据的整体思路：
（1）open函数打开CSV文件，模式为r，生成file对象。
（2）CSV模块的reader()方法加载对象file。
（3）使用reader（DictReader）读取数据。
（4）reader和DictReader区别：两者是接收一个可迭代的对象，返回一个生成器，reader函数是将一行数据以列表形式返回；DictReader函数返回的是一个字典，字典的值是单元格的值，而字典的键则是这个单元格的标题（即列头）

### 3.Excel写入数据的整体思路：

（1）xlwt创建生成临时Excel对象。
（2）添加WorkSheets对象。
（3）单元格的位置由行列索引决定，索引从0开始。
（4）数据写入主要由write merge0和write0实现，两者分别是合并单元格再写入和单元格写入。
（5）设置数据格式是在写入（write_merge()和write()）数据传入参数style。

### 4.Excel读取数据的整体思路：
（1）xlrd生成Workbook对象，并指向Excel文件。
（2）选择Workbook里某个WorkSheets对象。
（3）获取WorkSheets里数据已占用的总行数和总列数（某个单元格数据）。
（4）循环总行数和总列数，读取每一个单元格的数据。

### 5.Word写入数据的整体思路：
（1）创建生成临时Word对象并使用以下方法添加内容：
（2）add_heading()添加标题。
（3）add_paragraph()添加正文内容。
（4）add_picture()插入图片。
（5）add_table()添加表格。

### 6.Word读取数据的整体思路：
（1）生成Word对象，并指向Word文件。
（2）paragraphs()获取Word对象全部内容。
（3）循环paragraphs对象，获取每行数据并写入列表。
（4）将列表转换为字符串，每个列表元素使用换行符连接，转换后数据的段落布局与Word文档相似。

## ORM 框架



## MongoDB数据库操作



## 实战：爬取51Job招聘信息

爬虫配置文件（本书的17.6节）

**这一节的内容介绍很有实战教学意义。**

## 实战：分布式爬虫--QQ音乐

本章以QQ音乐为爬取对象，爬取范围是全站的歌曲信息，爬取方式在歌手列表获取每一位歌手的全部歌曲。如果爬取的数量较大，就使用异步编程实现分布式爬虫开发，可提高爬虫效率。
读者应重点掌握以下内容：

本章以QQ音乐为爬取对象，**爬取范围是全站的歌曲信息**，爬取方式在歌手列表获取每一位歌手的全部歌曲。如果爬取的数量较大，就使用异步编程实现分布式爬虫开发，可提高爬虫效率。
读者应重点掌握以下内容：

### 1.项目实现的功能
（1）download（guid，songmid，cookie dict）：歌曲下载，这是爬虫最底层的功能。
（2）get singer_songs（singermid，cookie dict）：将歌手的歌曲信息入库和歌曲下载。
（3）get genre singer（index，page list，cookie dict）：获取字母分类的全部歌手和歌曲信息。
（4）get_all_singer）：循环26个字母和特殊符号#，构建参数并调用函数get genre_singer。
（5）insert data（song dict）：将爬取的歌手和歌曲信息入库处理。
（6）myProcess()：每个字母分类创建一个单独进程运行。
（7）my Thread（index，cookie dict）：每个进程使用多线程爬取数据。

### 2.分布式策略考虑的因素
分布式策略考虑的因素有网站服务器负载量、网速快慢、硬件配置和数据库最大连接量。举个例子，爬取某个网站1000万数据，从数据量分析，当然进程和线程越多，爬取的速度越快。但往往忽略了网站服务器的并发量，假设设定10个进程，每个进程200条线程，每秒并发量为200×10 = 2000，若网站服务器并发量远远低于该并发量，在发送请求到网站的时候，就会出现卡死的情况，导致请求超时（即使对超时做了相应处理），无形之中增加了等待时间。除此之外，进程和线程越多，对运行爬虫程序的系统的压力越大，若涉及数据入库，还要考虑并发数是否超出数据库的最大连接数的情况。

### 3.实现分布式爬虫的注意事项
（1）全局变量不能放在ifname='main'中，因为使用多进程的时候，新开的进程不会在此获取数据。
（2）使用SQLalchemy入库最好重新创建一个数据库连接，如果多个线程和进程共同使用一个连接，就会出现异常。

分布式策略最好在程序代码的最外层实现。例如在项目中，函数 get singer_songs里有两个for循环，不建议在此使用分布式处理，在代码底层实现分布式不是不可行，只是代码变动太大，而且考虑的因素较多，代码维护相对较难。
**提示：**本章项目仅用于爬虫教学，并无违反版权法规之意，请读者注意自觉树立版权保护意识。